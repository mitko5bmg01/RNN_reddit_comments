{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca543b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e8a882f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dimitaryasenovoparlakov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dimitaryasenovoparlakov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/dimitaryasenovoparlakov/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n",
      "Found 63024 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'appointments' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "import csv\n",
    "import itertools\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print(\"Reading CSV file...\")\n",
    "with open('/Users/dimitaryasenovoparlakov/Documents/ML/RNN_reddit_comments/reddit-comments-2015-08.csv', 'r') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    next(reader)\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])\n",
    "\n",
    "# Create the training data\n",
    "X_train = [np.array([word_to_index[w] for w in sent[:-1]]) for sent in tokenized_sentences][0:200]\n",
    "y_train = [np.array([word_to_index[w] for w in sent[1:]]) for sent in tokenized_sentences]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5622d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "hyperparams = {\n",
    "    'vocab_size': 8000,\n",
    "    'hidden_dim': 8000,\n",
    "    'truncation': 5,\n",
    "    'num_epochs': 4,\n",
    "    'lr': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23a8e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hyperparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        vocab_size = hyperparams['vocab_size']\n",
    "        hidden_dim = hyperparams['hidden_dim']\n",
    "        max_u = np.sqrt(vocab_size)           \n",
    "        max_w = np.sqrt(hidden_dim)\n",
    "        \n",
    "        U_init = torch.rand(\n",
    "            hidden_dim, vocab_size,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        W_init = torch.rand(\n",
    "            hidden_dim, hidden_dim,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        U_init = U_init / (max_u / 2.0) - (2.0 / max_u)\n",
    "        W_init = W_init / (max_w / 2.0) - (2.0 / max_w)\n",
    "        b_init = 4.0 * torch.rand(hidden_dim, requires_grad=False) / max_w\n",
    "        \n",
    "        self.U = nn.parameter.Parameter(data=U_init, requires_grad=True)\n",
    "        self.W = nn.parameter.Parameter(data=W_init, requires_grad=True)\n",
    "        self.b = nn.parameter.Parameter(data=b_init, requires_grad=True)\n",
    "            \n",
    "        \n",
    "    def forward(self, s, x):\n",
    "        out = F.linear(s, self.W)\n",
    "        out += F.linear(x, self.U)\n",
    "        out += self.b\n",
    "        \n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "16dee5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bptt(model, sentences, hyperparams):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    adam = torch.optim.Adam(model.parameters(), lr=hyperparams['lr'])\n",
    "\n",
    "    for i in range(hyperparams['num_epochs']):\n",
    "        for k, sent in enumerate(sentences):\n",
    "            adam.zero_grad()\n",
    "            \n",
    "            s = torch.zeros(hyperparams['hidden_dim'])\n",
    "            ws = F.one_hot(torch.tensor(sent), num_classes=hyperparams['vocab_size'])\n",
    "            total_loss = torch.tensor(0.0)\n",
    "            \n",
    "            # print(sent.shape[0])\n",
    "            for j in range(sent.shape[0] - 1):\n",
    "                # print(j)\n",
    "                x = ws[j].float()\n",
    "                s = model(s, x)\n",
    "\n",
    "                if j % hyperparams['truncation'] == 0:\n",
    "                    s.detach()\n",
    "                \n",
    "                pred_distr = torch.unsqueeze(s, 0)\n",
    "                total_loss += loss_fn(pred_distr, torch.tensor([sent[j+1]]))\n",
    "            \n",
    "            total_loss /= sent.shape[0]\n",
    "            if k % 20 == 0:\n",
    "                print(k, total_loss)\n",
    "            total_loss.backward()\n",
    "            adam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9686503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(8.5377, grad_fn=<DivBackward0>)\n",
      "20 tensor(8.5552, grad_fn=<DivBackward0>)\n",
      "40 tensor(8.6233, grad_fn=<DivBackward0>)\n",
      "60 tensor(8.7755, grad_fn=<DivBackward0>)\n",
      "80 tensor(7.7035, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = RNN(hyperparams)\n",
    "\n",
    "train_bptt(model, X_train, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90572e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
